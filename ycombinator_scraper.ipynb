{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5421fb56",
   "metadata": {
    "id": "5421fb56"
   },
   "source": [
    "# Scraping Companies Information for listed  companies  on Ycombinator\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Project Outline\n",
    "1. To start with, I'm going to scrape about 1000 entries from https://ycombinator.com/companies, which are:\n",
    "- The listed company names\n",
    "- The company's ycombinator page url \n",
    "- The company location (will prefer to get it here since it is written with the country, unlike how it appeared when the company name has been clicked\n",
    "- The company description head/slogan. Then,\n",
    "\n",
    "\n",
    "![Untitled-2.png](attachment:Untitled-2.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. I'll go through the scraped company's ycombinator page url  and grab many other informations (company's description, year founded, team size, company page url, social media urls, management details) as they appear on the page.\n",
    "\n",
    "3. At the end, I will create for each company, a CSV file in the following format:\n",
    "\n",
    "```\n",
    "'Company_Name'| 'Company_Page_URL'| 'Company_Location'| 'Description_Head'| 'Website'| 'Description'| 'Founded'| 'Team_Size'| 'Linkedin_Profile'| 'Twitter_Profile'| 'Facebook_Profile'| 'Crunchbase_Profile'| 'Active_Founder1'| 'Active_Founder2'| 'Active_Founder3'\n",
    "Airbnb|\thttps://www.ycombinator.com/companies/airbnb|\tSan Francisco, CA, US,|\tBook accommodations around the world.|  http://airbnb.com | Founded in August of 2008 and based in San Fra... | 2008 | 5000 | https://www.linkedin.com/company/airbnb/ | https://twitter.com/Airbnb | https://www.facebook.com/airbnb/ | https://www.crunchbase.com/organization/airbnb | Nathan Blecharczyk\\nNone\\nhttps://twitter.com/... | Brian Chesky\\nNone\\nhttps://twitter.com/bchesky\\n | Joe Gebbia\\nNone\\nhttps://twitter.com/jgebbia\\n,```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d4d0b",
   "metadata": {
    "id": "196d4d0b"
   },
   "source": [
    "## Import necessary libraries\n",
    "\n",
    "- use **selenium** to downlaod the page\n",
    "- use **BS4** to parse and extract information\n",
    "- convert to a Pandas dataframe\n",
    "\n",
    "lets import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eMJzHprjtPb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMJzHprjtPb3",
    "outputId": "fcae54c3-0745-4b9e-ecfd-eb4e7ff3ee88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
      "\u001b[K     |████████████████████████████████| 968 kB 24.1 MB/s \n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "\u001b[K     |████████████████████████████████| 359 kB 56.0 MB/s \n",
      "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 53.6 MB/s \n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting pyOpenSSL>=0.14\n",
      "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Collecting cryptography>=1.3.4\n",
      "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 57.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
      "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed async-generator-1.10 cryptography-36.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25dc5d55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25dc5d55",
    "outputId": "1ca0ffd6-d03e-4566-c1e3-58a64d2b1c21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454d443",
   "metadata": {
    "id": "2454d443"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e46330",
   "metadata": {
    "id": "14e46330"
   },
   "outputs": [],
   "source": [
    "def chrome(mode='h'):\n",
    "\n",
    "    ''' A function to instantiate chrome driver\n",
    "\n",
    "        :arguments: \n",
    "            mode - representing either headless (preferred) or browser mode.\n",
    "        :returns: \n",
    "            driver - the driver object instantiated.\n",
    "    '''\n",
    "\n",
    "    if mode == 'h':\n",
    "        #  Headless mode\n",
    "        chrome_option = Options()\n",
    "        chrome_option.add_argument(\"--headless\")\n",
    "        chrome_option.add_argument(\"--log-level=3\")     # disabling unwanted messages printed while running with am headless browser\n",
    "        driver = webdriver.Chrome(options=chrome_option)\n",
    "\n",
    "    elif mode == 'b':\n",
    "        # Browser mode\n",
    "        driver = webdriver.Chrome()\n",
    "\n",
    "    else:\n",
    "        print(\"Mode is invalid\")\n",
    "        return None\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "def get_ycombinator_page_source(page_url=None, browser=None):\n",
    "    \"\"\"\n",
    "    A function to get the page source codes and contents\n",
    "\n",
    "        :arguments: \n",
    "            page_url - the url for of the page\n",
    "            browser - the webdriver object\n",
    "        :returns: \n",
    "            page_dom - a beautiful soup object of the page contents.\n",
    "    \"\"\"\n",
    "    \n",
    "    time.sleep(5)\n",
    "    browser.get(page_url)\n",
    "    time.sleep(20) # allow the page to load \n",
    "    \n",
    "    \n",
    "    ################ implement infinite scrolling ######################################\n",
    "    try:\n",
    "        \n",
    "        previous_height = browser.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            i+=1\n",
    "            print(\"scroll: \", i)\n",
    "\n",
    "            browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            new_height = browser.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "            if previous_height == new_height:\n",
    "                print(\"End of page reached\")\n",
    "                break\n",
    "\n",
    "            previous_height = new_height\n",
    "    \n",
    "    except:\n",
    "        browser.close()\n",
    "    ################################################################################\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    page_dom = BeautifulSoup(browser.page_source,\"html.parser\")\n",
    "    \n",
    "    return page_dom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_company_page_info(doc):\n",
    "    \"\"\"\n",
    "    A function to get the names of startup companies, location and url of company pages on ycombinator\n",
    "\n",
    "        :arguments: \n",
    "            doc - the bs4 object of the ycombinator page\n",
    "        :returns: \n",
    "            DataFrame object of the data collected\n",
    "    \"\"\"\n",
    "    page_dict = { 'Company_Name': [], 'Company_Page_URL': [], 'Company_Location': [], 'Description_Head': []}\n",
    "\n",
    "    # find all a tags with class name \"styles-module__company___1UVnl no-hovercard\" which points to individual company segment\n",
    "    item = doc.find_all(\"a\",{\"class\":\"styles-module__company___1UVnl no-hovercard\"})\n",
    "    \n",
    "    \n",
    "    # company ycompany's page url\n",
    "    url_list = []\n",
    "    \n",
    "    # get page informations\n",
    "    for i in range(len(item)):\n",
    "        # company name\n",
    "        company_name = item[i].find('span', {\"class\":\"styles-module__coName___3zz21\"})\n",
    "        page_dict[\"Company_Name\"].append(company_name.text)\n",
    "        \n",
    "        # company page url\n",
    "        company_page_url = item[i]['href']\n",
    "        page_dict[\"Company_Page_URL\"].append(\"https://www.ycombinator.com\"+company_page_url)\n",
    "        \n",
    "        # company ycompany's page url\n",
    "        url_list.append(\"https://www.ycombinator.com\"+company_page_url)\n",
    "\n",
    "        # company location\n",
    "        company_location = item[i].find('span', {\"class\":\"styles-module__coLocation___yhKam\"})\n",
    "        page_dict[\"Company_Location\"].append(company_location.text)\n",
    "        \n",
    "        # Description_Head\n",
    "        description_head = item[i].find('span', {\"class\":\"styles-module__coDescription___1b_yd\"})\n",
    "        page_dict[\"Description_Head\"].append(description_head.text)\n",
    "\n",
    "        \n",
    "    return pd.DataFrame(page_dict), url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b7033",
   "metadata": {
    "id": "9b1b7033"
   },
   "source": [
    "\n",
    "## Now, I will write functions to :\n",
    "\n",
    "\n",
    "1. Browse each Company_Page_URL \n",
    "2. Get actual company website address\n",
    "3. Get the company description as appeared on ycombinator\n",
    "4. Get the company year founded and team size,\n",
    "5. Get the company social media urls\n",
    "6. Get the company founder infos such as there name, position and social media urls\n",
    "7. Then after collecting the above info for all Company_Page_URL, create a CSV from concatenating the earlier scraped df with the dataframe of the newly collected infos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "289dfd5f",
   "metadata": {
    "id": "289dfd5f"
   },
   "outputs": [],
   "source": [
    "def get_company_website(doc):\n",
    "    # scrape the websites\n",
    "    try:\n",
    "        company_websites_tags = doc.find(\"a\",{\"target\":\"_blank\"})\n",
    "        try:\n",
    "            company_websites = company_websites_tags.text\n",
    "        except:\n",
    "            company_websites = company_websites_tags\n",
    "    except:\n",
    "        company_websites = np.nan\n",
    "    return company_websites\n",
    "\n",
    "\n",
    "\n",
    "def get_company_description(doc):\n",
    "    # scrape the descriptions\n",
    "    try:\n",
    "        company_description_tags = doc.find(\"p\",{\"class\":\"whitespace-pre-line\"})\n",
    "        try:\n",
    "            company_description = company_description_tags.text\n",
    "        except:\n",
    "            company_description = company_description_tags\n",
    "    except:\n",
    "        company_description = np.nan\n",
    "    return company_description\n",
    "\n",
    "\n",
    "\n",
    "def get_company_year_founded_and_team_size(doc):\n",
    "\n",
    "    # scrape\n",
    "    try:\n",
    "        ppty = doc.find(\"div\",{\"class\":\"space-y-0.5\"}).find_all(\"div\",{\"class\":\"flex flex-row justify-between\"})\n",
    "        year_founded = ppty[0].text.split(\":\")[1]\n",
    "        team_size = ppty[1].text.split(\":\")[1]\n",
    "        \n",
    "        company_year = year_founded\n",
    "        company_size = team_size\n",
    "    except:\n",
    "        company_year = np.nan\n",
    "        company_size = np.nan\n",
    "\n",
    "    return company_year, company_size\n",
    "\n",
    "\n",
    "        \n",
    "def get_company_social_media_urls(doc):\n",
    "\n",
    "    # scrape the social network urls\n",
    "    try:\n",
    "        \n",
    "        sm_info = doc.find(\"div\",{\"class\":\"space-x-2\"})\n",
    "        \n",
    "        try:\n",
    "            linkedin_url = sm_info.find(\"a\",{\"title\":\"LinkedIn profile\"})[\"href\"]\n",
    "        except:\n",
    "            linkedin_url = np.NaN     \n",
    "        try:\n",
    "            twitter_url = sm_info.find(\"a\",{\"title\":\"Twitter account\"})[\"href\"]\n",
    "        except:\n",
    "            twitter_url = np.NaN    \n",
    "        try:\n",
    "            facebook_url = sm_info.find(\"a\",{\"title\":\"Facebook profile\"})[\"href\"]\n",
    "        except:\n",
    "            facebook_url = np.NaN\n",
    "        try:\n",
    "            crunchbase_url = sm_info.find(\"a\",{\"title\":\"Crunchbase profile\"})[\"href\"]\n",
    "        except:\n",
    "            crunchbase_url = np.NaN\n",
    "\n",
    "    except:\n",
    "        linkedin_url = np.NaN\n",
    "        twitter_url = np.NaN    \n",
    "        facebook_url = np.NaN\n",
    "        crunchbase_url = np.NaN \n",
    "\n",
    "    return linkedin_url, twitter_url, facebook_url, crunchbase_url\n",
    "\n",
    "        \n",
    "def get_founder_info(doc):\n",
    "    founder_dict = {}\n",
    "\n",
    "    try:\n",
    "        founder_info = doc.find_all(\"div\",{\"class\":\"leading-snug\"})\n",
    "    except:\n",
    "        founder_info = \"\"\n",
    "    \n",
    "\n",
    "    if len(founder_info)>0:\n",
    "        \n",
    "        for i in range(len(founder_info)):\n",
    "\n",
    "            founder_name = founder_info[i].find(\"div\",{\"class\":\"font-bold\"}).text\n",
    "\n",
    "\n",
    "            try:\n",
    "                founder_post = founder_info[i].find(\"div\",{\"class\":\"\"}).text\n",
    "            except:\n",
    "                founder_post = founder_info[i].find(\"div\",{\"class\":\"\"})\n",
    "\n",
    "            try:\n",
    "                founder_sm = founder_info[i].find(\"div\",{\"class\":\"mt-1 space-x-2\"}).find_all(\"a\")\n",
    "                founder_sm_links = \"\"\n",
    "                for j in founder_sm:\n",
    "                    founder_sm_links = founder_sm_links + str(j[\"href\"]) + \"\\n\"\n",
    "            except:\n",
    "                founder_sm_links = \"None\"\n",
    "\n",
    "            founder = founder_name + \"\\n\" + str(founder_post) + \"\\n\"  + founder_sm_links # + \"\\n\" + str(founder_descr) \n",
    "\n",
    "            founder_dict[\"Active_Founder\"+str(i+1)] = founder\n",
    "\n",
    "        return founder_dict\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        return founder_dict\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def scrape_all(url=None):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception('Failed to load page {}'.format(url))\n",
    "        \n",
    "    soup_other = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "    c_web = get_company_website(soup_other)\n",
    "    c_description = get_company_description(soup_other)\n",
    "    company_year, company_size = get_company_year_founded_and_team_size(soup_other)\n",
    "    linkedin_url, twitter_url, facebook_url, crunchbase_url = get_company_social_media_urls(soup_other)\n",
    "    founder_details = get_founder_info(soup_other)\n",
    "\n",
    "\n",
    "    d = {'Website': c_web,\n",
    "         'Description': c_description,\n",
    "         'Founded': company_year,\n",
    "         'Team_Size': company_size,\n",
    "         'Linkedin_Profile': linkedin_url,\n",
    "         'Twitter_Profile': twitter_url,\n",
    "         'Facebook_Profile': facebook_url,\n",
    "         'Crunchbase_Profile': crunchbase_url,             \n",
    "         }     \n",
    "\n",
    "    d.update(founder_details)\n",
    "\n",
    "    \n",
    "\n",
    "    return d\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def make_dataframe_and_save(df1 = None, l=None):\n",
    "    final_df = pd.concat([df,pd.DataFrame(l)], axis=1)\n",
    "    \n",
    "    final_df.to_csv(\"ycombinator_data.csv\", index = False)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980019a6",
   "metadata": {
    "id": "980019a6"
   },
   "source": [
    "# Scrape the list of companies link, name, location and short description(head) from ycomobinator\n",
    "\n",
    "`chrome`: used to instantiate the webdriver as either a headless browser or not\n",
    "\n",
    "`get_ycombinator_page_source`: Used to handle the dynamic scraping of the project. It scrolls the website till it reaches the end of the page. Afterward, beautifulsoup is used to parse the page source which is used as input to:\n",
    "\n",
    "`get_company_page_info`: extracted the COmpany name, links, short description/description head and location of the companies. returns url_list and a dataframe for the already scraped information.\n",
    "\n",
    "\n",
    "**Note:** _They in total take `~2 minutes` to run_ and the lenght of url_list must be 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64d36f",
   "metadata": {
    "id": "fb64d36f",
    "outputId": "7dbf35c0-b7b4-419b-e03a-ba0c661f693f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scroll:  1\n",
      "scroll:  2\n",
      "scroll:  3\n",
      "scroll:  4\n",
      "scroll:  5\n",
      "scroll:  6\n",
      "scroll:  7\n",
      "scroll:  8\n",
      "scroll:  9\n",
      "scroll:  10\n",
      "scroll:  11\n",
      "scroll:  12\n",
      "scroll:  13\n",
      "scroll:  14\n",
      "scroll:  15\n",
      "scroll:  16\n",
      "scroll:  17\n",
      "scroll:  18\n",
      "scroll:  19\n",
      "scroll:  20\n",
      "scroll:  21\n",
      "scroll:  22\n",
      "scroll:  23\n",
      "scroll:  24\n",
      "scroll:  25\n",
      "scroll:  26\n",
      "scroll:  27\n",
      "scroll:  28\n",
      "scroll:  29\n",
      "scroll:  30\n",
      "scroll:  31\n",
      "scroll:  32\n",
      "scroll:  33\n",
      "scroll:  34\n",
      "scroll:  35\n",
      "scroll:  36\n",
      "scroll:  37\n",
      "scroll:  38\n",
      "scroll:  39\n",
      "scroll:  40\n",
      "scroll:  41\n",
      "scroll:  42\n",
      "scroll:  43\n",
      "scroll:  44\n",
      "scroll:  45\n",
      "scroll:  46\n",
      "scroll:  47\n",
      "scroll:  48\n",
      "scroll:  49\n",
      "End of page reached\n",
      "The time used is 117.5173134803772 seconds\n",
      "The length of url_list is 1000 \n"
     ]
    }
   ],
   "source": [
    "root_url = \" https://ycombinator.com/companies\"\n",
    "\n",
    "driver = chrome(\"h\")  # instantiate the webdriver\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "doc = get_ycombinator_page_source(page_url=root_url, browser = driver)\n",
    "\n",
    "\n",
    "df, url_list = get_company_page_info(doc)\n",
    "\n",
    "time_used = time.time() - time_start\n",
    "print(\"The time used is %s seconds\"%(time_used))\n",
    "\n",
    "url_list_len = len(url_list)\n",
    "print(\"The length of url_list is %s \"%(url_list_len))\n",
    "# close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752d888",
   "metadata": {
    "id": "9752d888"
   },
   "source": [
    "# B1. Scraping Companies' information  without multi-threading\n",
    "Runs for ~ 4 minutes with good network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb0b21b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fb0b21b",
    "outputId": "a48c83ea-bb12-4391-a647-409506bc204d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time used is 238.06634640693665 seconds\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "retries = []\n",
    "count = 1\n",
    "\n",
    "# start_no_thread = dt.now()\n",
    "time_start = time.time()\n",
    "\n",
    "run = True\n",
    "while run:\n",
    "    for link in url_list:\n",
    "        try:\n",
    "            print(f\"{count}/{url_list_len}\", end='\\r')\n",
    "            d = scrape_all(url=link)\n",
    "            l.append(d)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            retries.append(link)\n",
    "    \n",
    "    if retries != []:\n",
    "        url_list = retries\n",
    "        retries = []\n",
    "    else:\n",
    "        run = False\n",
    "        \n",
    "\n",
    "# runtime_no_thread = (dt.now() - start_no_thread).total_seconds()\n",
    "# print(f'Total runtime - {runtime_no_thread}')\n",
    "time_used = time.time() - time_start\n",
    "print(\"The time used is %s seconds\"%(time_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177bb76",
   "metadata": {
    "id": "4177bb76"
   },
   "source": [
    "#### View and safe  the company information DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a6678ce",
   "metadata": {
    "id": "8a6678ce"
   },
   "outputs": [],
   "source": [
    "df_final = make_dataframe_and_save(df1 = df, l=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "NW7tYyUMxAvb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NW7tYyUMxAvb",
    "outputId": "38cd7f49-b175-4bd1-9c21-e8f73a4447c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_Name            0\n",
       "Company_Page_URL        0\n",
       "Company_Location       16\n",
       "Description_Head        4\n",
       "Website                 0\n",
       "Description             0\n",
       "Founded                 0\n",
       "Team_Size               0\n",
       "Linkedin_Profile      272\n",
       "Twitter_Profile       350\n",
       "Facebook_Profile      613\n",
       "Crunchbase_Profile    357\n",
       "Active_Founder1        13\n",
       "Active_Founder2       211\n",
       "Active_Founder3       758\n",
       "Active_Founder4       949\n",
       "Active_Founder5       993\n",
       "Active_Founder6       999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df96543",
   "metadata": {
    "id": "5df96543"
   },
   "source": [
    "# B2. Scraping Companies' information  WITH multi-threading\n",
    "\n",
    "Runs for 36 seconds with good network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867cc8cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "867cc8cf",
    "outputId": "794029ac-8f80-436b-cd86-6f3581e71e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime - 35.944235\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures as cf\n",
    "\n",
    "\n",
    "start_thread = dt.now()\n",
    "l1 = []\n",
    "with cf.ThreadPoolExecutor() as exc:\n",
    "    results = exc.map(scrape_all, url_list)\n",
    "\n",
    "    for result in results:\n",
    "        l1.append(result)\n",
    "\n",
    "runtime_thread = (dt.now() - start_thread).total_seconds()\n",
    "print(f'Total runtime - {runtime_thread}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7369e",
   "metadata": {},
   "source": [
    "#### View and safe  the company information DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "-XCVMQAEzzWf",
   "metadata": {
    "id": "-XCVMQAEzzWf"
   },
   "outputs": [],
   "source": [
    "df_final_threaded = make_dataframe_and_save(df1 = df, l=l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "VpBTX3zFz6J_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpBTX3zFz6J_",
    "outputId": "667ff099-8037-48df-b5ff-f33e36c5437e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_Name            0\n",
       "Company_Page_URL        0\n",
       "Company_Location       16\n",
       "Description_Head        4\n",
       "Website                 0\n",
       "Description             0\n",
       "Founded                 0\n",
       "Team_Size               0\n",
       "Linkedin_Profile      272\n",
       "Twitter_Profile       350\n",
       "Facebook_Profile      613\n",
       "Crunchbase_Profile    357\n",
       "Active_Founder1        13\n",
       "Active_Founder2       211\n",
       "Active_Founder3       758\n",
       "Active_Founder4       949\n",
       "Active_Founder5       993\n",
       "Active_Founder6       999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_threaded.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0a626",
   "metadata": {
    "id": "77e0a626"
   },
   "source": [
    "# References and Future Work\n",
    "\n",
    "### Summary of what I did / Issues\n",
    "- I have just succesfully done all the outlined procedures from the begiining of the project.\n",
    "- The script runs on the assumption that the Internet would be available for the script to finish running.\n",
    "\n",
    "### References\n",
    "\n",
    "- How to make infinite scrolling with selenium [https://www.youtube.com/watch?v=qhJ_gMB772U]\n",
    " \n",
    "### Ideas for future work\n",
    "\n",
    "- Analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a2d17",
   "metadata": {
    "id": "641a2d17"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ycombinator_scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
